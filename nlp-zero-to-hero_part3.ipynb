{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NLP Zero to Hero\n\n## Introduction to NLP\n\nNatural Language Processing (NLP) is a sub-field of artificial intelligence (AI) that focuses on the interaction between computers and humans through natural language. The ultimate objective of NLP is to enable computers to understand, interpret, and generate human languages in a way that is both meaningful and useful.\n\n### Why is NLP Important?\n\nNLP is important because it helps resolve ambiguity in language and adds useful numeric representation to the data for many downstream applications, such as text analytics or speech recognition. Here are some reasons why NLP is important:\n\n1. **Volume of Text Data**: With the explosion of digital communication, the amount of text data generated daily is vast. NLP helps in extracting useful information from this vast amount of unstructured data.\n2. **Human-Computer Interaction**: NLP provides more natural interactions between humans and computers, making technologies like virtual assistants and chatbots more effective.\n3. **Automation of Routine Tasks**: NLP can automate and align routine tasks such as summarizing documents, filtering spam emails, and translating languages.\n\n### Applications of NLP\n\nNLP has a wide range of applications across various domains:\n\n- **Text Classification**: Categorizing text into predefined categories. For example, filtering spam emails or classifying customer reviews.\n- **Sentiment Analysis**: Determining the sentiment expressed in a piece of text, such as identifying positive or negative reviews.\n- **Machine Translation**: Translating text from one language to another, like Google Translate.\n- **Named Entity Recognition (NER)**: Identifying entities such as names, dates, and places within a text.\n- **Speech Recognition**: Converting spoken language into text, as used in virtual assistants like Siri and Alexa.\n- **Chatbots**: Enabling conversational agents to understand and respond to human queries in real-time.\n- **Text Summarization**: Creating a summary of a longer piece of text.\n","metadata":{}},{"cell_type":"markdown","source":"### Basic NLP Concepts and Terminology\n\nBefore looking into NLP tasks, it's essential to understand some basic concepts and terminology:\n\n- **Tokenization**: The process of splitting text into individual words or phrases, known as tokens.\n- **Stopwords**: Common words like \"the\", \"is\", \"in\", which are often removed from text before processing because they add little value to the analysis.\n- **Stemming and Lemmatization**: Techniques to reduce words to their base or root form. Stemming is a crude heuristic process that chops off the ends of words, while lemmatization uses a dictionary to find the root form.\n- **Vectorization**: Converting text into numerical format. Common methods include Bag of Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF).\n- **Word Embeddings**: Dense vector representations of words, capturing their meanings, semantic relationships, and context. Examples include Word2Vec, GloVe, and FastText.\n- **Sequence Models**: Models like Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM), and Gated Recurrent Units (GRUs) that are capable of processing sequences of data.\n- **Transformers**: A type of model architecture that has revolutionized NLP, particularly through models like BERT, GPT, and T5. Transformers handle sequences in parallel and have shown significant improvements in various NLP tasks.\n\n### Structure of This Notebook\n\nThis notebook will take you through a journey from basic NLP tasks to more advanced techniques. Hereâ€™s the structure:\n\n1. **Text Preprocessing**\n   - Tokenization\n   - Stopword removal\n   - Stemming and Lemmatization\n   - Vectorization\n\n2. **Basic NLP Tasks**\n   - Sentiment Analysis\n   - Named Entity Recognition (NER)\n   - Part-of-Speech Tagging\n   - Text Classification\n\n3. **Advanced NLP Techniques**\n   - Word Embeddings (Word2Vec, GloVe)\n   - Sequence Models (RNN, LSTM, GRU)\n   - Attention Mechanisms and Transformers\n   - BERT and other Transformer-based models\n\n4. **Practical Projects**\n   - Building a Sentiment Analysis Model\n   - Creating a Chatbot\n   - Text Summarization\n   - Machine Translation\n\n5. **Conclusion and Further Reading**\n   - Summary of key points\n   - Resources for further learning\n\nLet's get started on this exciting journey into the world of Natural Language Processing!","metadata":{}},{"cell_type":"markdown","source":"# Part 1 : Text Preprocessing","metadata":{}},{"cell_type":"markdown","source":"### ğŸ“ Importing libraries","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nimport spacy","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:29.339609Z","iopub.execute_input":"2024-06-13T03:54:29.340026Z","iopub.status.idle":"2024-06-13T03:54:29.346391Z","shell.execute_reply.started":"2024-06-13T03:54:29.339992Z","shell.execute_reply":"2024-06-13T03:54:29.344890Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"#### ğŸš€ Let's download NLTK and Spacy data we need (just once)","metadata":{}},{"cell_type":"code","source":"!python -m spacy download en_core_web_md","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:29.351746Z","iopub.execute_input":"2024-06-13T03:54:29.352165Z","iopub.status.idle":"2024-06-13T03:54:49.160729Z","shell.execute_reply.started":"2024-06-13T03:54:29.352130Z","shell.execute_reply":"2024-06-13T03:54:49.159161Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Collecting en-core-web-md==3.7.1\n  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl (42.8 MB)\nRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /opt/conda/lib/python3.10/site-packages (from en-core-web-md==3.7.1) (3.7.4)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.2.3)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\nRequirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.3.4)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.9.0)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (6.4.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.66.1)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.31.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.5.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (69.0.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (21.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.0)\nRequirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.26.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.1.1)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.14.6)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2024.2.2)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.11)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.4)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.1.7)\nRequirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.1.3)\n\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_md')\n","output_type":"stream"}]},{"cell_type":"code","source":"nltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\nnlp = spacy.load('en_core_web_sm')\n# nltk.download('omw-1.4')","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:49.163056Z","iopub.execute_input":"2024-06-13T03:54:49.163449Z","iopub.status.idle":"2024-06-13T03:54:50.401769Z","shell.execute_reply.started":"2024-06-13T03:54:49.163412Z","shell.execute_reply":"2024-06-13T03:54:50.400768Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"# ğŸŒŸ Example text\ntext = \"\"\"\nFeatureLens generates visuals by applying text mining concepts such as frequent words, expressions, and closed itemsets of n-grams to guide the discovery process. These concepts are combined with interactive visualization to help users analyze text, create insights\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.403152Z","iopub.execute_input":"2024-06-13T03:54:50.403532Z","iopub.status.idle":"2024-06-13T03:54:50.408926Z","shell.execute_reply.started":"2024-06-13T03:54:50.403482Z","shell.execute_reply":"2024-06-13T03:54:50.407863Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"### ğŸª„ Step 1: Tokenization - Breaking text into words (tokens) ğŸŒŸ","metadata":{}},{"cell_type":"code","source":"tokens = word_tokenize(text)\nprint(\"ğŸ”¹ Tokens:\", tokens)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.411480Z","iopub.execute_input":"2024-06-13T03:54:50.412532Z","iopub.status.idle":"2024-06-13T03:54:50.423867Z","shell.execute_reply.started":"2024-06-13T03:54:50.412483Z","shell.execute_reply":"2024-06-13T03:54:50.422760Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"ğŸ”¹ Tokens: ['FeatureLens', 'generates', 'visuals', 'by', 'applying', 'text', 'mining', 'concepts', 'such', 'as', 'frequent', 'words', ',', 'expressions', ',', 'and', 'closed', 'itemsets', 'of', 'n-grams', 'to', 'guide', 'the', 'discovery', 'process', '.', 'These', 'concepts', 'are', 'combined', 'with', 'interactive', 'visualization', 'to', 'help', 'users', 'analyze', 'text', ',', 'create', 'insights']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### ğŸ›‘ Step 2: Stopword removal - Get rid of those pesky common words ğŸ›‘","metadata":{}},{"cell_type":"code","source":"\nstop_words = set(stopwords.words('english'))\nfiltered_tokens = [word for word in tokens if word.lower() not in stop_words]\nprint(\"ğŸ”¹ Tokens after stopword removal:\", filtered_tokens)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.425072Z","iopub.execute_input":"2024-06-13T03:54:50.425385Z","iopub.status.idle":"2024-06-13T03:54:50.439646Z","shell.execute_reply.started":"2024-06-13T03:54:50.425357Z","shell.execute_reply":"2024-06-13T03:54:50.438500Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"ğŸ”¹ Tokens after stopword removal: ['FeatureLens', 'generates', 'visuals', 'applying', 'text', 'mining', 'concepts', 'frequent', 'words', ',', 'expressions', ',', 'closed', 'itemsets', 'n-grams', 'guide', 'discovery', 'process', '.', 'concepts', 'combined', 'interactive', 'visualization', 'help', 'users', 'analyze', 'text', ',', 'create', 'insights']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### âœ‚ï¸ Step 3: Stemming - Chopping words to their roots using PorterStemmer! âœ‚ï¸","metadata":{}},{"cell_type":"code","source":"stemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\nprint(\"ğŸ”¹ Stemmed tokens:\", stemmed_tokens)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.440902Z","iopub.execute_input":"2024-06-13T03:54:50.441224Z","iopub.status.idle":"2024-06-13T03:54:50.457798Z","shell.execute_reply.started":"2024-06-13T03:54:50.441197Z","shell.execute_reply":"2024-06-13T03:54:50.456813Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"ğŸ”¹ Stemmed tokens: ['featurelen', 'gener', 'visual', 'appli', 'text', 'mine', 'concept', 'frequent', 'word', ',', 'express', ',', 'close', 'itemset', 'n-gram', 'guid', 'discoveri', 'process', '.', 'concept', 'combin', 'interact', 'visual', 'help', 'user', 'analyz', 'text', ',', 'creat', 'insight']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### ğŸŒ± Step 4: Lemmatization - Morphing words to their base form with WordNetLemmatizer! ğŸŒ±","metadata":{}},{"cell_type":"code","source":"def lemmatize_with_spacy(tokens):\n    doc = nlp(' '.join(tokens))\n    return [token.lemma_ for token in doc]\n\nlemmatized_tokens = lemmatize_with_spacy(filtered_tokens)\nprint(\"ğŸ”¹ Lemmatized tokens:\", lemmatized_tokens)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.459011Z","iopub.execute_input":"2024-06-13T03:54:50.459354Z","iopub.status.idle":"2024-06-13T03:54:50.495195Z","shell.execute_reply.started":"2024-06-13T03:54:50.459319Z","shell.execute_reply":"2024-06-13T03:54:50.494118Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"ğŸ”¹ Lemmatized tokens: ['FeatureLens', 'generate', 'visual', 'apply', 'text', 'mining', 'concept', 'frequent', 'word', ',', 'expression', ',', 'closed', 'itemset', 'n', '-', 'gram', 'guide', 'discovery', 'process', '.', 'concept', 'combine', 'interactive', 'visualization', 'help', 'user', 'analyze', 'text', ',', 'create', 'insight']\n","output_type":"stream"}]},{"cell_type":"code","source":"# lemmatizer = WordNetLemmatizer()\n# lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n# print(\"ğŸ”¹ Lemmatized tokens:\", lemmatized_tokens)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.496486Z","iopub.execute_input":"2024-06-13T03:54:50.497065Z","iopub.status.idle":"2024-06-13T03:54:50.502877Z","shell.execute_reply.started":"2024-06-13T03:54:50.497034Z","shell.execute_reply":"2024-06-13T03:54:50.501822Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"### ğŸ’¼ Step 5: Vectorization - Turn those words into numbers for our model! ğŸ“Š","metadata":{}},{"cell_type":"markdown","source":"#### ğŸ§® Count Vectorizer - Counting word occurrences","metadata":{}},{"cell_type":"code","source":"count_vectorizer = CountVectorizer()\ncount_vectors = count_vectorizer.fit_transform([' '.join(lemmatized_tokens)])\nprint(\"ğŸ”¹ Count Vectorizer feature names:\", count_vectorizer.get_feature_names_out())\nprint(\"ğŸ”¹ Count Vectors:\\n\", count_vectors.toarray())","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.504206Z","iopub.execute_input":"2024-06-13T03:54:50.504601Z","iopub.status.idle":"2024-06-13T03:54:50.519183Z","shell.execute_reply.started":"2024-06-13T03:54:50.504564Z","shell.execute_reply":"2024-06-13T03:54:50.517701Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"ğŸ”¹ Count Vectorizer feature names: ['analyze' 'apply' 'closed' 'combine' 'concept' 'create' 'discovery'\n 'expression' 'featurelens' 'frequent' 'generate' 'gram' 'guide' 'help'\n 'insight' 'interactive' 'itemset' 'mining' 'process' 'text' 'user'\n 'visual' 'visualization' 'word']\nğŸ”¹ Count Vectors:\n [[1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### ğŸ”¥ TF-IDF Vectorizer - Considering word frequency across documents","metadata":{}},{"cell_type":"code","source":"tfidf_vectorizer = TfidfVectorizer()\ntfidf_vectors = tfidf_vectorizer.fit_transform([' '.join(lemmatized_tokens)])\nprint(\"ğŸ”¹ TF-IDF Vectorizer feature names:\", tfidf_vectorizer.get_feature_names_out())\nprint(\"ğŸ”¹ TF-IDF Vectors:\\n\", tfidf_vectors.toarray())","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.523635Z","iopub.execute_input":"2024-06-13T03:54:50.523994Z","iopub.status.idle":"2024-06-13T03:54:50.537235Z","shell.execute_reply.started":"2024-06-13T03:54:50.523962Z","shell.execute_reply":"2024-06-13T03:54:50.536086Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"ğŸ”¹ TF-IDF Vectorizer feature names: ['analyze' 'apply' 'closed' 'combine' 'concept' 'create' 'discovery'\n 'expression' 'featurelens' 'frequent' 'generate' 'gram' 'guide' 'help'\n 'insight' 'interactive' 'itemset' 'mining' 'process' 'text' 'user'\n 'visual' 'visualization' 'word']\nğŸ”¹ TF-IDF Vectors:\n [[0.18257419 0.18257419 0.18257419 0.18257419 0.36514837 0.18257419\n  0.18257419 0.18257419 0.18257419 0.18257419 0.18257419 0.18257419\n  0.18257419 0.18257419 0.18257419 0.18257419 0.18257419 0.18257419\n  0.18257419 0.36514837 0.18257419 0.18257419 0.18257419 0.18257419]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### ğŸ‰ And that's it guys! Our text is now preprocessed and ready for some NLP magic! ğŸ‰","metadata":{}},{"cell_type":"markdown","source":"# Part 2 : Basic NLP Tasks","metadata":{}},{"cell_type":"markdown","source":"####  ğŸ‰ Now, let's dive into some basic NLP tasks! ğŸ‰","metadata":{}},{"cell_type":"markdown","source":"### ğŸ“¢ Task 1: Sentiment Analysis - Get those text vibes! ğŸ˜ƒğŸ˜¢ğŸ˜¡","metadata":{}},{"cell_type":"code","source":"# NLTK's VADER for sentiment analysis\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nsid = SentimentIntensityAnalyzer()\n\nsentiment_scores = sid.polarity_scores(text)\nprint(\"ğŸ”¹ Sentiment Scores:\", sentiment_scores)\n\ndef get_sentiment_label(scores):\n    if scores['compound'] >= 0.05:\n        return 'positive'\n    elif scores['compound'] <= -0.05:\n        return 'negative'\n    else:\n        return 'neutral'\n    \nsentiment_label = get_sentiment_label(sentiment_scores)\nprint(f\"ğŸ”¹ Sentiment Label: {sentiment_label}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.538446Z","iopub.execute_input":"2024-06-13T03:54:50.538899Z","iopub.status.idle":"2024-06-13T03:54:50.563092Z","shell.execute_reply.started":"2024-06-13T03:54:50.538858Z","shell.execute_reply":"2024-06-13T03:54:50.561811Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"ğŸ”¹ Sentiment Scores: {'neg': 0.0, 'neu': 0.879, 'pos': 0.121, 'compound': 0.5859}\nğŸ”¹ Sentiment Label: positive\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### ğŸ­ Task 2: Named Entity Recognition (NER) - Who's who and what's what? ğŸ¤”","metadata":{}},{"cell_type":"code","source":"# using spaCy\ndoc = nlp(text)\n\nentities = [(entity.text, entity.label_) for entity in doc.ents]\nprint(\"ğŸ”¹ Named Entities:\", entities)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.565156Z","iopub.execute_input":"2024-06-13T03:54:50.565573Z","iopub.status.idle":"2024-06-13T03:54:50.592971Z","shell.execute_reply.started":"2024-06-13T03:54:50.565533Z","shell.execute_reply":"2024-06-13T03:54:50.591893Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"ğŸ”¹ Named Entities: [('FeatureLens', 'ORG')]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### ğŸ·ï¸ Task 3: Part-of-Speech Tagging - Tagging words like a pro! ğŸ·ï¸","metadata":{}},{"cell_type":"code","source":"pos_tags = [(token.text, token.pos_) for token in doc]\nprint(\"ğŸ”¹ Part-of-Speech Tags:\", pos_tags)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.594256Z","iopub.execute_input":"2024-06-13T03:54:50.594605Z","iopub.status.idle":"2024-06-13T03:54:50.600646Z","shell.execute_reply.started":"2024-06-13T03:54:50.594575Z","shell.execute_reply":"2024-06-13T03:54:50.599577Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"ğŸ”¹ Part-of-Speech Tags: [('\\n', 'SPACE'), ('FeatureLens', 'PROPN'), ('generates', 'VERB'), ('visuals', 'NOUN'), ('by', 'ADP'), ('applying', 'VERB'), ('text', 'NOUN'), ('mining', 'NOUN'), ('concepts', 'NOUN'), ('such', 'ADJ'), ('as', 'ADP'), ('frequent', 'ADJ'), ('words', 'NOUN'), (',', 'PUNCT'), ('expressions', 'NOUN'), (',', 'PUNCT'), ('and', 'CCONJ'), ('closed', 'ADJ'), ('itemsets', 'NOUN'), ('of', 'ADP'), ('n', 'NOUN'), ('-', 'PUNCT'), ('grams', 'NOUN'), ('to', 'PART'), ('guide', 'VERB'), ('the', 'DET'), ('discovery', 'NOUN'), ('process', 'NOUN'), ('.', 'PUNCT'), ('These', 'DET'), ('concepts', 'NOUN'), ('are', 'AUX'), ('combined', 'VERB'), ('with', 'ADP'), ('interactive', 'ADJ'), ('visualization', 'NOUN'), ('to', 'PART'), ('help', 'VERB'), ('users', 'NOUN'), ('analyze', 'VERB'), ('text', 'NOUN'), (',', 'PUNCT'), ('create', 'VERB'), ('insights', 'NOUN'), ('\\n', 'SPACE')]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### ğŸ“š Task 4: Text Classification - Let's train a simple classifier! ğŸ“š","metadata":{}},{"cell_type":"code","source":"train_texts = [\n    \"I love sunny days\",\n    \"I hate rainy days\",\n    \"Sunshine makes me happy\",\n    \"Rainy days make me sad\"\n]\ntrain_labels = ['positive', 'negative', 'positive', 'negative']","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.602106Z","iopub.execute_input":"2024-06-13T03:54:50.602445Z","iopub.status.idle":"2024-06-13T03:54:50.616930Z","shell.execute_reply.started":"2024-06-13T03:54:50.602410Z","shell.execute_reply":"2024-06-13T03:54:50.615880Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"#### ğŸ§‘â€ğŸ« Step 1: Vectorize the training texts","metadata":{}},{"cell_type":"code","source":"vectorizer = CountVectorizer()\nX_train = vectorizer.fit_transform(train_texts)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.618385Z","iopub.execute_input":"2024-06-13T03:54:50.618919Z","iopub.status.idle":"2024-06-13T03:54:50.631422Z","shell.execute_reply.started":"2024-06-13T03:54:50.618880Z","shell.execute_reply":"2024-06-13T03:54:50.630445Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"#### ğŸ§  Step 2: Train a simple classifier (Naive Bayes)","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\nclassifier = MultinomialNB()\nclassifier.fit(X_train, train_labels)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.632438Z","iopub.execute_input":"2024-06-13T03:54:50.632799Z","iopub.status.idle":"2024-06-13T03:54:50.650437Z","shell.execute_reply.started":"2024-06-13T03:54:50.632771Z","shell.execute_reply":"2024-06-13T03:54:50.649310Z"},"trusted":true},"execution_count":54,"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"MultinomialNB()","text/html":"<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"markdown","source":"#### ğŸ“ Step 3: Predict sentiment of a new text","metadata":{}},{"cell_type":"code","source":"new_text = \"I feel happy when it is sunny\"\nX_new = vectorizer.transform([new_text])\npredicted_label = classifier.predict(X_new)[0]\nprint(f\"ğŸ”¹ New Text: {new_text}\")\nprint(f\"ğŸ”¹ Predicted Sentiment: {predicted_label}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.651808Z","iopub.execute_input":"2024-06-13T03:54:50.652176Z","iopub.status.idle":"2024-06-13T03:54:50.663724Z","shell.execute_reply.started":"2024-06-13T03:54:50.652145Z","shell.execute_reply":"2024-06-13T03:54:50.662328Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"ğŸ”¹ New Text: I feel happy when it is sunny\nğŸ”¹ Predicted Sentiment: positive\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### ğŸ‰ And that's a wrap on our basic NLP tasks! You're now an NLP hero! ğŸ‰","metadata":{}},{"cell_type":"markdown","source":"# Part 3 : Advanced NLP Techniques","metadata":{}},{"cell_type":"markdown","source":"## Sub-part 1  : Word Embeddings (Word2Vec, GloVe) ğŸŒ","metadata":{}},{"cell_type":"markdown","source":"####  ğŸ—£ï¸ Word Embeddings are like word vibes! They capture semantic meaning by representing words as vectors.\n#### ğŸš€ First up, let's explore Word2Vec using gensim! ğŸ§ ","metadata":{}},{"cell_type":"code","source":"import gensim\nfrom gensim.models import Word2Vec\n\nsentences = [\n    \"I love sunny days\",\n    \"I hate rainy days\",\n    \"Sunshine makes me happy\",\n    \"Rainy days make me sad\"\n]","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.665022Z","iopub.execute_input":"2024-06-13T03:54:50.665486Z","iopub.status.idle":"2024-06-13T03:54:50.677110Z","shell.execute_reply.started":"2024-06-13T03:54:50.665455Z","shell.execute_reply":"2024-06-13T03:54:50.676050Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"#### ğŸª„ Step 1: Tokenization (we did this before, but let's do it again for completeness)","metadata":{}},{"cell_type":"code","source":"tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\nprint(\"ğŸ”¹ Tokenized Sentences:\", tokenized_sentences)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.678333Z","iopub.execute_input":"2024-06-13T03:54:50.679017Z","iopub.status.idle":"2024-06-13T03:54:50.690041Z","shell.execute_reply.started":"2024-06-13T03:54:50.678961Z","shell.execute_reply":"2024-06-13T03:54:50.689038Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"ğŸ”¹ Tokenized Sentences: [['i', 'love', 'sunny', 'days'], ['i', 'hate', 'rainy', 'days'], ['sunshine', 'makes', 'me', 'happy'], ['rainy', 'days', 'make', 'me', 'sad']]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### ğŸ§  Step 2: Train the Word2Vec model","metadata":{}},{"cell_type":"code","source":"word2vec_model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\nprint(\"ğŸ”¹ Word2Vec Model Trained!\")","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.691232Z","iopub.execute_input":"2024-06-13T03:54:50.691600Z","iopub.status.idle":"2024-06-13T03:54:50.713661Z","shell.execute_reply.started":"2024-06-13T03:54:50.691566Z","shell.execute_reply":"2024-06-13T03:54:50.712603Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"ğŸ”¹ Word2Vec Model Trained!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### ğŸ” Step 3: Explore Word2Vec embeddings","metadata":{}},{"cell_type":"code","source":"word = \"sunny\"\nif word in word2vec_model.wv:\n    print(f\"ğŸ”¹ Vector for '{word}':\", word2vec_model.wv[word])\nelse:\n    print(f\"ğŸ”¹ Word '{word}' not in vocabulary\")","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.715013Z","iopub.execute_input":"2024-06-13T03:54:50.715675Z","iopub.status.idle":"2024-06-13T03:54:50.723796Z","shell.execute_reply.started":"2024-06-13T03:54:50.715644Z","shell.execute_reply":"2024-06-13T03:54:50.722731Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"ğŸ”¹ Vector for 'sunny': [ 7.0887972e-03 -1.5679300e-03  7.9474989e-03 -9.4886590e-03\n -8.0294991e-03 -6.6403709e-03 -4.0034545e-03  4.9892161e-03\n -3.8135587e-03 -8.3199050e-03  8.4117772e-03 -3.7470020e-03\n  8.6086961e-03 -4.8957514e-03  3.9185942e-03  4.9220170e-03\n  2.3926091e-03 -2.8188038e-03  2.8491246e-03 -8.2562361e-03\n -2.7655398e-03 -2.5911583e-03  7.2490061e-03 -3.4634031e-03\n -6.5997029e-03  4.3404270e-03 -4.7448516e-04 -3.5975564e-03\n  6.8824720e-03  3.8723124e-03 -3.9002013e-03  7.7188847e-04\n  9.1435025e-03  7.7546560e-03  6.3618720e-03  4.6673026e-03\n  2.3844899e-03 -1.8416261e-03 -6.3712932e-03 -3.0181051e-04\n -1.5653884e-03 -5.7228567e-04 -6.2628710e-03  7.4340473e-03\n -6.5914928e-03 -7.2392775e-03 -2.7571463e-03 -1.5154004e-03\n -7.6357173e-03  6.9824100e-04 -5.3261113e-03 -1.2755442e-03\n -7.3651113e-03  1.9605684e-03  3.2731986e-03 -2.3138524e-05\n -5.4483581e-03 -1.7260861e-03  7.0849168e-03  3.7362587e-03\n -8.8810492e-03 -3.4135508e-03  2.3541022e-03  2.1380198e-03\n -9.4640078e-03  4.5711659e-03 -8.6569972e-03 -7.3870681e-03\n  3.4831120e-03 -3.4709584e-03  3.5644709e-03  8.8940905e-03\n -3.5743224e-03  9.3204249e-03  1.7110384e-03  9.8477742e-03\n  5.7050432e-03 -9.1494834e-03 -3.3277308e-03  6.5301750e-03\n  5.6027793e-03  8.7055154e-03  6.9261026e-03  8.0388878e-03\n -9.8230084e-03  4.2988253e-03 -5.0300765e-03  3.5123860e-03\n  6.0566878e-03  4.3921317e-03  7.5123594e-03  1.4977157e-03\n -1.2649416e-03  5.7684006e-03 -5.6395675e-03  3.8591625e-05\n  9.4565870e-03 -5.4812501e-03  3.8142789e-03 -8.1130210e-03]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### ğŸ‘¯â€â™‚ï¸ Find similar words","metadata":{}},{"cell_type":"code","source":"similar_words = word2vec_model.wv.most_similar(word, topn=5)\nprint(f\"ğŸ”¹ Words similar to '{word}':\", similar_words)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.725469Z","iopub.execute_input":"2024-06-13T03:54:50.725879Z","iopub.status.idle":"2024-06-13T03:54:50.735943Z","shell.execute_reply.started":"2024-06-13T03:54:50.725849Z","shell.execute_reply":"2024-06-13T03:54:50.734729Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"ğŸ”¹ Words similar to 'sunny': [('love', 0.10941850394010544), ('makes', 0.10889014601707458), ('days', 0.06285077333450317), ('happy', 0.05048205703496933), ('sunshine', 0.026806795969605446)]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### ğŸŒ Now, let's explore GloVe embeddings with spaCy! ğŸ§ \n#### ğŸ“¢ If you are testing this notebook, ,ake sure to download and test the larger spaCy model for better results:\n#### python -m spacy download en_core_web_md","metadata":{}},{"cell_type":"code","source":"nlp_glove = spacy.load('en_core_web_md')","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.737246Z","iopub.execute_input":"2024-06-13T03:54:50.737637Z","iopub.status.idle":"2024-06-13T03:54:53.547084Z","shell.execute_reply.started":"2024-06-13T03:54:50.737601Z","shell.execute_reply":"2024-06-13T03:54:53.546002Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"#### ğŸ” Step 4: Explore GloVe embeddings","metadata":{}},{"cell_type":"code","source":"doc = nlp_glove(\"I love sunny days\")\nfor token in doc:\n    print(f\"ğŸ”¹ Token: {token.text}, Vector: {token.vector[:5]}...\")  # Show first 5 dimensions for brevity","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:53.548469Z","iopub.execute_input":"2024-06-13T03:54:53.548846Z","iopub.status.idle":"2024-06-13T03:54:53.566545Z","shell.execute_reply.started":"2024-06-13T03:54:53.548815Z","shell.execute_reply":"2024-06-13T03:54:53.565387Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"ğŸ”¹ Token: I, Vector: [ -1.8607    0.15804  -4.1425   -8.6359  -16.955  ]...\nğŸ”¹ Token: love, Vector: [ 2.0565  -3.2259  -5.7364  -6.146    0.15748]...\nğŸ”¹ Token: sunny, Vector: [-0.14498  0.65651 -2.8764   0.40125 -1.0028 ]...\nğŸ”¹ Token: days, Vector: [-3.0808  4.6691  1.1    -1.3048  4.1861]...\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### ğŸ‘¯â€â™‚ï¸ Find similar words using spaCy's GloVe embeddings","metadata":{}},{"cell_type":"code","source":"def find_similar_words_spacy(word):\n    token = nlp_glove.vocab[word]\n    queries = [w for w in token.vocab if w.is_lower == token.is_lower and w.prob >= -15]\n    by_similarity = sorted(queries, key=lambda w: token.similarity(w), reverse=True)\n    return [w.text for w in by_similarity[:5]]\n\nword = \"sunny\"\nsimilar_words_glove = find_similar_words_spacy(word)\nprint(f\"ğŸ”¹ Words similar to '{word}' using GloVe:\", similar_words_glove)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:53.567962Z","iopub.execute_input":"2024-06-13T03:54:53.568354Z","iopub.status.idle":"2024-06-13T03:54:53.578756Z","shell.execute_reply.started":"2024-06-13T03:54:53.568319Z","shell.execute_reply":"2024-06-13T03:54:53.577583Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"ğŸ”¹ Words similar to 'sunny' using GloVe: []\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### ğŸ‰ And that's a wrap on Word Embeddings! You're now a Word2Vec and GloVe pro! ğŸ‰","metadata":{}},{"cell_type":"markdown","source":"## Sub-part 2 : Sequence Models (RNN, LSTM, GRU) ğŸŒ","metadata":{}},{"cell_type":"markdown","source":"#### ğŸ‰ Get ready to ride the wave of Sequence Models! ğŸŒŠ","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, GRU, Dense\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:53.580264Z","iopub.execute_input":"2024-06-13T03:54:53.580702Z","iopub.status.idle":"2024-06-13T03:55:07.184396Z","shell.execute_reply.started":"2024-06-13T03:54:53.580653Z","shell.execute_reply":"2024-06-13T03:55:07.182670Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stderr","text":"2024-06-13 03:54:55.730968: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-13 03:54:55.731101: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-13 03:54:55.917712: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# ğŸŒŸ Sample sentences for sequence modeling (replace with your dataset)\nsentences = [\n    \"I love sunny days\",\n    \"I hate rainy days\",\n    \"Sunshine makes me happy\",\n    \"Rainy days make me sad\",\n    \"I feel great on sunny days\",\n    \"Rainy days are gloomy and sad\"\n]","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:55:07.186018Z","iopub.execute_input":"2024-06-13T03:55:07.186892Z","iopub.status.idle":"2024-06-13T03:55:07.192727Z","shell.execute_reply.started":"2024-06-13T03:55:07.186858Z","shell.execute_reply":"2024-06-13T03:55:07.191420Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":"#### ğŸ¯ Step 1: Tokenization and Padding","metadata":{}},{"cell_type":"code","source":"max_vocab_size = 1000\nmax_sequence_length = 10\n\n# Tokenize the sentences\ntokenizer = Tokenizer(num_words=max_vocab_size)\ntokenizer.fit_on_texts(sentences)\nsequences = tokenizer.texts_to_sequences(sentences)\nword_index = tokenizer.word_index\n\n# Pad the sequences\npadded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\nprint(\"ğŸ”¹ Padded Sequences:\\n\", padded_sequences)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:55:07.200793Z","iopub.execute_input":"2024-06-13T03:55:07.201365Z","iopub.status.idle":"2024-06-13T03:55:07.230144Z","shell.execute_reply.started":"2024-06-13T03:55:07.201315Z","shell.execute_reply":"2024-06-13T03:55:07.228787Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"ğŸ”¹ Padded Sequences:\n [[ 2  7  4  1  0  0  0  0  0  0]\n [ 2  8  3  1  0  0  0  0  0  0]\n [ 9 10  5 11  0  0  0  0  0  0]\n [ 3  1 12  5  6  0  0  0  0  0]\n [ 2 13 14 15  4  1  0  0  0  0]\n [ 3  1 16 17 18  6  0  0  0  0]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### ğŸ¢ Step 2: Preparing the labels","metadata":{}},{"cell_type":"code","source":"labels = np.array([1, 0, 1, 0, 1, 0])  # Positive: 1, Negative: 0","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:55:07.231406Z","iopub.execute_input":"2024-06-13T03:55:07.231767Z","iopub.status.idle":"2024-06-13T03:55:07.244341Z","shell.execute_reply.started":"2024-06-13T03:55:07.231738Z","shell.execute_reply":"2024-06-13T03:55:07.243023Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"#### ğŸš€ Step 3: Build and train a Simple RNN model","metadata":{}},{"cell_type":"code","source":"print(\"ğŸš€ Training a Simple RNN model...\")\n\nrnn_model = Sequential([\n    Embedding(input_dim=max_vocab_size, output_dim=32, input_length=max_sequence_length),\n    SimpleRNN(32, return_sequences=False),\n    Dense(1, activation='sigmoid')\n])\n\nrnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nrnn_model.summary()\n\n# Train the RNN model\nrnn_model.fit(padded_sequences, labels, epochs=5, verbose=2)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:55:07.245931Z","iopub.execute_input":"2024-06-13T03:55:07.246379Z","iopub.status.idle":"2024-06-13T03:55:09.804385Z","shell.execute_reply.started":"2024-06-13T03:55:07.246340Z","shell.execute_reply":"2024-06-13T03:55:09.803288Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"ğŸš€ Training a Simple RNN model...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:86: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)          â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)          â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/5\n1/1 - 2s - 2s/step - accuracy: 0.3333 - loss: 0.7038\nEpoch 2/5\n1/1 - 0s - 26ms/step - accuracy: 0.6667 - loss: 0.6854\nEpoch 3/5\n1/1 - 0s - 26ms/step - accuracy: 1.0000 - loss: 0.6676\nEpoch 4/5\n1/1 - 0s - 26ms/step - accuracy: 1.0000 - loss: 0.6498\nEpoch 5/5\n1/1 - 0s - 27ms/step - accuracy: 1.0000 - loss: 0.6317\n","output_type":"stream"},{"execution_count":68,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7e7ec31e9ed0>"},"metadata":{}}]},{"cell_type":"markdown","source":"#### ğŸš€ Step 4: Build and train an LSTM model","metadata":{}},{"cell_type":"code","source":"print(\"ğŸš€ Training an LSTM model...\")\n\nlstm_model = Sequential([\n    Embedding(input_dim=max_vocab_size, output_dim=32, input_length=max_sequence_length),\n    LSTM(32, return_sequences=False),\n    Dense(1, activation='sigmoid')\n])\n\nlstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nlstm_model.summary()\n\n# Train the LSTM model\nlstm_model.fit(padded_sequences, labels, epochs=5, verbose=2)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:55:09.805960Z","iopub.execute_input":"2024-06-13T03:55:09.806398Z","iopub.status.idle":"2024-06-13T03:55:12.428163Z","shell.execute_reply.started":"2024-06-13T03:55:09.806360Z","shell.execute_reply":"2024-06-13T03:55:12.427040Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stdout","text":"ğŸš€ Training an LSTM model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/5\n1/1 - 2s - 2s/step - accuracy: 0.5000 - loss: 0.6931\nEpoch 2/5\n1/1 - 0s - 28ms/step - accuracy: 0.5000 - loss: 0.6926\nEpoch 3/5\n1/1 - 0s - 28ms/step - accuracy: 0.6667 - loss: 0.6921\nEpoch 4/5\n1/1 - 0s - 28ms/step - accuracy: 1.0000 - loss: 0.6915\nEpoch 5/5\n1/1 - 0s - 27ms/step - accuracy: 0.6667 - loss: 0.6910\n","output_type":"stream"},{"execution_count":69,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7e7ebe52d1b0>"},"metadata":{}}]},{"cell_type":"markdown","source":"#### ğŸš€ Step 5: Build and train a GRU model","metadata":{}},{"cell_type":"code","source":"print(\"ğŸš€ Training a GRU model...\")\n\ngru_model = Sequential([\n    Embedding(input_dim=max_vocab_size, output_dim=32, input_length=max_sequence_length),\n    GRU(32, return_sequences=False),\n    Dense(1, activation='sigmoid')\n])\n\ngru_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\ngru_model.summary()\n\ngru_model.fit(padded_sequences, labels, epochs=5, verbose=2)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:55:12.429611Z","iopub.execute_input":"2024-06-13T03:55:12.429976Z","iopub.status.idle":"2024-06-13T03:55:15.357754Z","shell.execute_reply.started":"2024-06-13T03:55:12.429946Z","shell.execute_reply":"2024-06-13T03:55:15.356580Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"ğŸš€ Training a GRU model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_2\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ gru (\u001b[38;5;33mGRU\u001b[0m)                       â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/5\n1/1 - 3s - 3s/step - accuracy: 0.5000 - loss: 0.6930\nEpoch 2/5\n1/1 - 0s - 29ms/step - accuracy: 0.5000 - loss: 0.6928\nEpoch 3/5\n1/1 - 0s - 59ms/step - accuracy: 0.5000 - loss: 0.6925\nEpoch 4/5\n1/1 - 0s - 28ms/step - accuracy: 0.8333 - loss: 0.6923\nEpoch 5/5\n1/1 - 0s - 28ms/step - accuracy: 0.6667 - loss: 0.6920\n","output_type":"stream"},{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7e7ec0498d60>"},"metadata":{}}]},{"cell_type":"markdown","source":"#### ğŸ‰ Comparing Model Performances","metadata":{}},{"cell_type":"code","source":"# Evaluate the models\nrnn_loss, rnn_accuracy = rnn_model.evaluate(padded_sequences, labels, verbose=0)\nlstm_loss, lstm_accuracy = lstm_model.evaluate(padded_sequences, labels, verbose=0)\ngru_loss, gru_accuracy = gru_model.evaluate(padded_sequences, labels, verbose=0)\n\nprint(f\"ğŸ”¹ RNN Model Accuracy: {rnn_accuracy:.2f}\")\nprint(f\"ğŸ”¹ LSTM Model Accuracy: {lstm_accuracy:.2f}\")\nprint(f\"ğŸ”¹ GRU Model Accuracy: {gru_accuracy:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:55:15.359401Z","iopub.execute_input":"2024-06-13T03:55:15.359879Z","iopub.status.idle":"2024-06-13T03:55:16.408653Z","shell.execute_reply.started":"2024-06-13T03:55:15.359839Z","shell.execute_reply":"2024-06-13T03:55:16.407274Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":"ğŸ”¹ RNN Model Accuracy: 1.00\nğŸ”¹ LSTM Model Accuracy: 0.67\nğŸ”¹ GRU Model Accuracy: 0.67\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### ğŸ‰ And that's a wrap on Sequence Models! You're now an RNN, LSTM, and GRU pro! ğŸ‰","metadata":{}}]}