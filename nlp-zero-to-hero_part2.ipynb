{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NLP Zero to Hero\n\n## Introduction to NLP\n\nNatural Language Processing (NLP) is a sub-field of artificial intelligence (AI) that focuses on the interaction between computers and humans through natural language. The ultimate objective of NLP is to enable computers to understand, interpret, and generate human languages in a way that is both meaningful and useful.\n\n### Why is NLP Important?\n\nNLP is important because it helps resolve ambiguity in language and adds useful numeric representation to the data for many downstream applications, such as text analytics or speech recognition. Here are some reasons why NLP is important:\n\n1. **Volume of Text Data**: With the explosion of digital communication, the amount of text data generated daily is vast. NLP helps in extracting useful information from this vast amount of unstructured data.\n2. **Human-Computer Interaction**: NLP provides more natural interactions between humans and computers, making technologies like virtual assistants and chatbots more effective.\n3. **Automation of Routine Tasks**: NLP can automate and align routine tasks such as summarizing documents, filtering spam emails, and translating languages.\n\n### Applications of NLP\n\nNLP has a wide range of applications across various domains:\n\n- **Text Classification**: Categorizing text into predefined categories. For example, filtering spam emails or classifying customer reviews.\n- **Sentiment Analysis**: Determining the sentiment expressed in a piece of text, such as identifying positive or negative reviews.\n- **Machine Translation**: Translating text from one language to another, like Google Translate.\n- **Named Entity Recognition (NER)**: Identifying entities such as names, dates, and places within a text.\n- **Speech Recognition**: Converting spoken language into text, as used in virtual assistants like Siri and Alexa.\n- **Chatbots**: Enabling conversational agents to understand and respond to human queries in real-time.\n- **Text Summarization**: Creating a summary of a longer piece of text.\n","metadata":{}},{"cell_type":"markdown","source":"### Basic NLP Concepts and Terminology\n\nBefore looking into NLP tasks, it's essential to understand some basic concepts and terminology:\n\n- **Tokenization**: The process of splitting text into individual words or phrases, known as tokens.\n- **Stopwords**: Common words like \"the\", \"is\", \"in\", which are often removed from text before processing because they add little value to the analysis.\n- **Stemming and Lemmatization**: Techniques to reduce words to their base or root form. Stemming is a crude heuristic process that chops off the ends of words, while lemmatization uses a dictionary to find the root form.\n- **Vectorization**: Converting text into numerical format. Common methods include Bag of Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF).\n- **Word Embeddings**: Dense vector representations of words, capturing their meanings, semantic relationships, and context. Examples include Word2Vec, GloVe, and FastText.\n- **Sequence Models**: Models like Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM), and Gated Recurrent Units (GRUs) that are capable of processing sequences of data.\n- **Transformers**: A type of model architecture that has revolutionized NLP, particularly through models like BERT, GPT, and T5. Transformers handle sequences in parallel and have shown significant improvements in various NLP tasks.\n\n### Structure of This Notebook\n\nThis notebook will take you through a journey from basic NLP tasks to more advanced techniques. Hereâ€™s the structure:\n\n1. **Text Preprocessing**\n   - Tokenization\n   - Stopword removal\n   - Stemming and Lemmatization\n   - Vectorization\n\n2. **Basic NLP Tasks**\n   - Sentiment Analysis\n   - Named Entity Recognition (NER)\n   - Part-of-Speech Tagging\n   - Text Classification\n\n3. **Advanced NLP Techniques**\n   - Word Embeddings (Word2Vec, GloVe)\n   - Sequence Models (RNN, LSTM, GRU)\n   - Attention Mechanisms and Transformers\n   - BERT and other Transformer-based models\n\n4. **Practical Projects**\n   - Building a Sentiment Analysis Model\n   - Creating a Chatbot\n   - Text Summarization\n   - Machine Translation\n\n5. **Conclusion and Further Reading**\n   - Summary of key points\n   - Resources for further learning\n\nLet's get started on this exciting journey into the world of Natural Language Processing!","metadata":{}},{"cell_type":"markdown","source":"# Part 1 : Text Preprocessing","metadata":{}},{"cell_type":"markdown","source":"### ðŸ“ Importing libraries","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nimport spacy","metadata":{"execution":{"iopub.status.busy":"2024-06-12T04:02:59.257376Z","iopub.execute_input":"2024-06-12T04:02:59.258190Z","iopub.status.idle":"2024-06-12T04:03:05.173368Z","shell.execute_reply.started":"2024-06-12T04:02:59.258148Z","shell.execute_reply":"2024-06-12T04:03:05.171602Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"#### ðŸš€ Let's download NLTK and Spacy data we need (just once)","metadata":{}},{"cell_type":"code","source":"nltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\nnlp = spacy.load('en_core_web_sm')\n# nltk.download('omw-1.4')","metadata":{"execution":{"iopub.status.busy":"2024-06-12T04:05:04.962859Z","iopub.execute_input":"2024-06-12T04:05:04.963394Z","iopub.status.idle":"2024-06-12T04:05:06.762413Z","shell.execute_reply.started":"2024-06-12T04:05:04.963340Z","shell.execute_reply":"2024-06-12T04:05:06.760912Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"# ðŸŒŸ Example text\ntext = \"\"\"\nFeatureLens generates visuals by applying text mining concepts such as frequent words, expressions, and closed itemsets of n-grams to guide the discovery process. These concepts are combined with interactive visualization to help users analyze text, create insights\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-06-12T04:01:58.424603Z","iopub.execute_input":"2024-06-12T04:01:58.425056Z","iopub.status.idle":"2024-06-12T04:01:58.438323Z","shell.execute_reply.started":"2024-06-12T04:01:58.425009Z","shell.execute_reply":"2024-06-12T04:01:58.436863Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### ðŸª„ Step 1: Tokenization - Breaking text into words (tokens) ðŸŒŸ","metadata":{}},{"cell_type":"code","source":"tokens = word_tokenize(text)\nprint(\"ðŸ”¹ Tokens:\", tokens)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T04:01:58.440271Z","iopub.execute_input":"2024-06-12T04:01:58.440709Z","iopub.status.idle":"2024-06-12T04:01:58.456710Z","shell.execute_reply.started":"2024-06-12T04:01:58.440676Z","shell.execute_reply":"2024-06-12T04:01:58.455050Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"ðŸ”¹ Tokens: ['FeatureLens', 'generates', 'visuals', 'by', 'applying', 'text', 'mining', 'concepts', 'such', 'as', 'frequent', 'words', ',', 'expressions', ',', 'and', 'closed', 'itemsets', 'of', 'n-grams', 'to', 'guide', 'the', 'discovery', 'process', '.', 'These', 'concepts', 'are', 'combined', 'with', 'interactive', 'visualization', 'to', 'help', 'users', 'analyze', 'text', ',', 'create', 'insights']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### ðŸ›‘ Step 2: Stopword removal - Get rid of those pesky common words ðŸ›‘","metadata":{}},{"cell_type":"code","source":"\nstop_words = set(stopwords.words('english'))\nfiltered_tokens = [word for word in tokens if word.lower() not in stop_words]\nprint(\"ðŸ”¹ Tokens after stopword removal:\", filtered_tokens)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-12T04:01:58.460703Z","iopub.execute_input":"2024-06-12T04:01:58.461948Z","iopub.status.idle":"2024-06-12T04:01:58.472503Z","shell.execute_reply.started":"2024-06-12T04:01:58.461907Z","shell.execute_reply":"2024-06-12T04:01:58.470762Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"ðŸ”¹ Tokens after stopword removal: ['FeatureLens', 'generates', 'visuals', 'applying', 'text', 'mining', 'concepts', 'frequent', 'words', ',', 'expressions', ',', 'closed', 'itemsets', 'n-grams', 'guide', 'discovery', 'process', '.', 'concepts', 'combined', 'interactive', 'visualization', 'help', 'users', 'analyze', 'text', ',', 'create', 'insights']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### âœ‚ï¸ Step 3: Stemming - Chopping words to their roots using PorterStemmer! âœ‚ï¸","metadata":{}},{"cell_type":"code","source":"stemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\nprint(\"ðŸ”¹ Stemmed tokens:\", stemmed_tokens)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T04:01:58.474284Z","iopub.execute_input":"2024-06-12T04:01:58.474715Z","iopub.status.idle":"2024-06-12T04:01:58.491925Z","shell.execute_reply.started":"2024-06-12T04:01:58.474676Z","shell.execute_reply":"2024-06-12T04:01:58.490524Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"ðŸ”¹ Stemmed tokens: ['featurelen', 'gener', 'visual', 'appli', 'text', 'mine', 'concept', 'frequent', 'word', ',', 'express', ',', 'close', 'itemset', 'n-gram', 'guid', 'discoveri', 'process', '.', 'concept', 'combin', 'interact', 'visual', 'help', 'user', 'analyz', 'text', ',', 'creat', 'insight']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### ðŸŒ± Step 4: Lemmatization - Morphing words to their base form with WordNetLemmatizer! ðŸŒ±","metadata":{}},{"cell_type":"code","source":"def lemmatize_with_spacy(tokens):\n    doc = nlp(' '.join(tokens))\n    return [token.lemma_ for token in doc]\n\nlemmatized_tokens = lemmatize_with_spacy(filtered_tokens)\nprint(\"ðŸ”¹ Lemmatized tokens:\", lemmatized_tokens)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T04:05:09.478145Z","iopub.execute_input":"2024-06-12T04:05:09.478723Z","iopub.status.idle":"2024-06-12T04:05:09.534221Z","shell.execute_reply.started":"2024-06-12T04:05:09.478669Z","shell.execute_reply":"2024-06-12T04:05:09.532482Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"ðŸ”¹ Lemmatized tokens: ['FeatureLens', 'generate', 'visual', 'apply', 'text', 'mining', 'concept', 'frequent', 'word', ',', 'expression', ',', 'closed', 'itemset', 'n', '-', 'gram', 'guide', 'discovery', 'process', '.', 'concept', 'combine', 'interactive', 'visualization', 'help', 'user', 'analyze', 'text', ',', 'create', 'insight']\n","output_type":"stream"}]},{"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\nlemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\nprint(\"ðŸ”¹ Lemmatized tokens:\", lemmatized_tokens)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T04:01:58.560957Z","iopub.status.idle":"2024-06-12T04:01:58.561545Z","shell.execute_reply.started":"2024-06-12T04:01:58.561247Z","shell.execute_reply":"2024-06-12T04:01:58.561269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ðŸ’¼ Step 5: Vectorization - Turn those words into numbers for our model! ðŸ“Š","metadata":{}},{"cell_type":"markdown","source":"#### ðŸ§® Count Vectorizer - Counting word occurrences","metadata":{}},{"cell_type":"code","source":"count_vectorizer = CountVectorizer()\ncount_vectors = count_vectorizer.fit_transform([' '.join(lemmatized_tokens)])\nprint(\"ðŸ”¹ Count Vectorizer feature names:\", count_vectorizer.get_feature_names_out())\nprint(\"ðŸ”¹ Count Vectors:\\n\", count_vectors.toarray())","metadata":{"execution":{"iopub.status.busy":"2024-06-12T04:05:32.175459Z","iopub.execute_input":"2024-06-12T04:05:32.176136Z","iopub.status.idle":"2024-06-12T04:05:32.197980Z","shell.execute_reply.started":"2024-06-12T04:05:32.176093Z","shell.execute_reply":"2024-06-12T04:05:32.196216Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"ðŸ”¹ Count Vectorizer feature names: ['analyze' 'apply' 'closed' 'combine' 'concept' 'create' 'discovery'\n 'expression' 'featurelens' 'frequent' 'generate' 'gram' 'guide' 'help'\n 'insight' 'interactive' 'itemset' 'mining' 'process' 'text' 'user'\n 'visual' 'visualization' 'word']\nðŸ”¹ Count Vectors:\n [[1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### ðŸ”¥ TF-IDF Vectorizer - Considering word frequency across documents","metadata":{}},{"cell_type":"code","source":"tfidf_vectorizer = TfidfVectorizer()\ntfidf_vectors = tfidf_vectorizer.fit_transform([' '.join(lemmatized_tokens)])\nprint(\"ðŸ”¹ TF-IDF Vectorizer feature names:\", tfidf_vectorizer.get_feature_names_out())\nprint(\"ðŸ”¹ TF-IDF Vectors:\\n\", tfidf_vectors.toarray())","metadata":{"execution":{"iopub.status.busy":"2024-06-12T04:05:51.084648Z","iopub.execute_input":"2024-06-12T04:05:51.085198Z","iopub.status.idle":"2024-06-12T04:05:51.106265Z","shell.execute_reply.started":"2024-06-12T04:05:51.085162Z","shell.execute_reply":"2024-06-12T04:05:51.104770Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"ðŸ”¹ TF-IDF Vectorizer feature names: ['analyze' 'apply' 'closed' 'combine' 'concept' 'create' 'discovery'\n 'expression' 'featurelens' 'frequent' 'generate' 'gram' 'guide' 'help'\n 'insight' 'interactive' 'itemset' 'mining' 'process' 'text' 'user'\n 'visual' 'visualization' 'word']\nðŸ”¹ TF-IDF Vectors:\n [[0.18257419 0.18257419 0.18257419 0.18257419 0.36514837 0.18257419\n  0.18257419 0.18257419 0.18257419 0.18257419 0.18257419 0.18257419\n  0.18257419 0.18257419 0.18257419 0.18257419 0.18257419 0.18257419\n  0.18257419 0.36514837 0.18257419 0.18257419 0.18257419 0.18257419]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### ðŸŽ‰ And that's it guys! Our text is now preprocessed and ready for some NLP magic! ðŸŽ‰","metadata":{}},{"cell_type":"markdown","source":"# Part 2 : Basic NLP Tasks","metadata":{}},{"cell_type":"markdown","source":"####  ðŸŽ‰ Now, let's dive into some basic NLP tasks! ðŸŽ‰","metadata":{}},{"cell_type":"markdown","source":"### ðŸ“¢ Task 1: Sentiment Analysis - Get those text vibes! ðŸ˜ƒðŸ˜¢ðŸ˜¡","metadata":{}},{"cell_type":"code","source":"# NLTK's VADER for sentiment analysis\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nsid = SentimentIntensityAnalyzer()\n\nsentiment_scores = sid.polarity_scores(text)\nprint(\"ðŸ”¹ Sentiment Scores:\", sentiment_scores)\n\ndef get_sentiment_label(scores):\n    if scores['compound'] >= 0.05:\n        return 'positive'\n    elif scores['compound'] <= -0.05:\n        return 'negative'\n    else:\n        return 'neutral'\n    \nsentiment_label = get_sentiment_label(sentiment_scores)\nprint(f\"ðŸ”¹ Sentiment Label: {sentiment_label}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-12T04:19:16.465468Z","iopub.execute_input":"2024-06-12T04:19:16.465906Z","iopub.status.idle":"2024-06-12T04:19:16.489495Z","shell.execute_reply.started":"2024-06-12T04:19:16.465875Z","shell.execute_reply":"2024-06-12T04:19:16.487807Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"ðŸ”¹ Sentiment Scores: {'neg': 0.0, 'neu': 0.879, 'pos': 0.121, 'compound': 0.5859}\nðŸ”¹ Sentiment Label: positive\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### ðŸŽ­ Task 2: Named Entity Recognition (NER) - Who's who and what's what? ðŸ¤”","metadata":{}},{"cell_type":"code","source":"# using spaCy\ndoc = nlp(text)\n\nentities = [(entity.text, entity.label_) for entity in doc.ents]\nprint(\"ðŸ”¹ Named Entities:\", entities)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T04:15:49.484924Z","iopub.execute_input":"2024-06-12T04:15:49.485420Z","iopub.status.idle":"2024-06-12T04:15:49.519385Z","shell.execute_reply.started":"2024-06-12T04:15:49.485384Z","shell.execute_reply":"2024-06-12T04:15:49.517336Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"ðŸ”¹ Named Entities: [('FeatureLens', 'ORG')]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### ðŸ·ï¸ Task 3: Part-of-Speech Tagging - Tagging words like a pro! ðŸ·ï¸","metadata":{}},{"cell_type":"code","source":"pos_tags = [(token.text, token.pos_) for token in doc]\nprint(\"ðŸ”¹ Part-of-Speech Tags:\", pos_tags)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T04:16:21.740761Z","iopub.execute_input":"2024-06-12T04:16:21.741238Z","iopub.status.idle":"2024-06-12T04:16:21.750718Z","shell.execute_reply.started":"2024-06-12T04:16:21.741204Z","shell.execute_reply":"2024-06-12T04:16:21.749589Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"ðŸ”¹ Part-of-Speech Tags: [('\\n', 'SPACE'), ('FeatureLens', 'PROPN'), ('generates', 'VERB'), ('visuals', 'NOUN'), ('by', 'ADP'), ('applying', 'VERB'), ('text', 'NOUN'), ('mining', 'NOUN'), ('concepts', 'NOUN'), ('such', 'ADJ'), ('as', 'ADP'), ('frequent', 'ADJ'), ('words', 'NOUN'), (',', 'PUNCT'), ('expressions', 'NOUN'), (',', 'PUNCT'), ('and', 'CCONJ'), ('closed', 'ADJ'), ('itemsets', 'NOUN'), ('of', 'ADP'), ('n', 'NOUN'), ('-', 'PUNCT'), ('grams', 'NOUN'), ('to', 'PART'), ('guide', 'VERB'), ('the', 'DET'), ('discovery', 'NOUN'), ('process', 'NOUN'), ('.', 'PUNCT'), ('These', 'DET'), ('concepts', 'NOUN'), ('are', 'AUX'), ('combined', 'VERB'), ('with', 'ADP'), ('interactive', 'ADJ'), ('visualization', 'NOUN'), ('to', 'PART'), ('help', 'VERB'), ('users', 'NOUN'), ('analyze', 'VERB'), ('text', 'NOUN'), (',', 'PUNCT'), ('create', 'VERB'), ('insights', 'NOUN'), ('\\n', 'SPACE')]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### ðŸ“š Task 4: Text Classification - Let's train a simple classifier! ðŸ“š","metadata":{}},{"cell_type":"code","source":"train_texts = [\n    \"I love sunny days\",\n    \"I hate rainy days\",\n    \"Sunshine makes me happy\",\n    \"Rainy days make me sad\"\n]\ntrain_labels = ['positive', 'negative', 'positive', 'negative']","metadata":{"execution":{"iopub.status.busy":"2024-06-12T04:17:20.891587Z","iopub.execute_input":"2024-06-12T04:17:20.892141Z","iopub.status.idle":"2024-06-12T04:17:20.899920Z","shell.execute_reply.started":"2024-06-12T04:17:20.892108Z","shell.execute_reply":"2024-06-12T04:17:20.898091Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"#### ðŸ§‘â€ðŸ« Step 1: Vectorize the training texts","metadata":{}},{"cell_type":"code","source":"vectorizer = CountVectorizer()\nX_train = vectorizer.fit_transform(train_texts)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T04:17:47.713161Z","iopub.execute_input":"2024-06-12T04:17:47.713592Z","iopub.status.idle":"2024-06-12T04:17:47.723142Z","shell.execute_reply.started":"2024-06-12T04:17:47.713560Z","shell.execute_reply":"2024-06-12T04:17:47.721473Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"#### ðŸ§  Step 2: Train a simple classifier (Naive Bayes)","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\nclassifier = MultinomialNB()\nclassifier.fit(X_train, train_labels)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T04:18:20.114725Z","iopub.execute_input":"2024-06-12T04:18:20.115251Z","iopub.status.idle":"2024-06-12T04:18:20.134501Z","shell.execute_reply.started":"2024-06-12T04:18:20.115217Z","shell.execute_reply":"2024-06-12T04:18:20.132863Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"MultinomialNB()","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"markdown","source":"#### ðŸŽ“ Step 3: Predict sentiment of a new text","metadata":{}},{"cell_type":"code","source":"new_text = \"I feel happy when it is sunny\"\nX_new = vectorizer.transform([new_text])\npredicted_label = classifier.predict(X_new)[0]\nprint(f\"ðŸ”¹ New Text: {new_text}\")\nprint(f\"ðŸ”¹ Predicted Sentiment: {predicted_label}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-12T04:18:43.634370Z","iopub.execute_input":"2024-06-12T04:18:43.634930Z","iopub.status.idle":"2024-06-12T04:18:43.644870Z","shell.execute_reply.started":"2024-06-12T04:18:43.634889Z","shell.execute_reply":"2024-06-12T04:18:43.642920Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"ðŸ”¹ New Text: I feel happy when it is sunny\nðŸ”¹ Predicted Sentiment: positive\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### ðŸŽ‰ And that's a wrap on our basic NLP tasks! You're now an NLP hero! ðŸŽ‰","metadata":{}}]}