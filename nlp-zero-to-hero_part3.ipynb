{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NLP Zero to Hero\n\n## Introduction to NLP\n\nNatural Language Processing (NLP) is a sub-field of artificial intelligence (AI) that focuses on the interaction between computers and humans through natural language. The ultimate objective of NLP is to enable computers to understand, interpret, and generate human languages in a way that is both meaningful and useful.\n\n### Why is NLP Important?\n\nNLP is important because it helps resolve ambiguity in language and adds useful numeric representation to the data for many downstream applications, such as text analytics or speech recognition. Here are some reasons why NLP is important:\n\n1. **Volume of Text Data**: With the explosion of digital communication, the amount of text data generated daily is vast. NLP helps in extracting useful information from this vast amount of unstructured data.\n2. **Human-Computer Interaction**: NLP provides more natural interactions between humans and computers, making technologies like virtual assistants and chatbots more effective.\n3. **Automation of Routine Tasks**: NLP can automate and align routine tasks such as summarizing documents, filtering spam emails, and translating languages.\n\n### Applications of NLP\n\nNLP has a wide range of applications across various domains:\n\n- **Text Classification**: Categorizing text into predefined categories. For example, filtering spam emails or classifying customer reviews.\n- **Sentiment Analysis**: Determining the sentiment expressed in a piece of text, such as identifying positive or negative reviews.\n- **Machine Translation**: Translating text from one language to another, like Google Translate.\n- **Named Entity Recognition (NER)**: Identifying entities such as names, dates, and places within a text.\n- **Speech Recognition**: Converting spoken language into text, as used in virtual assistants like Siri and Alexa.\n- **Chatbots**: Enabling conversational agents to understand and respond to human queries in real-time.\n- **Text Summarization**: Creating a summary of a longer piece of text.\n","metadata":{}},{"cell_type":"markdown","source":"### Basic NLP Concepts and Terminology\n\nBefore looking into NLP tasks, it's essential to understand some basic concepts and terminology:\n\n- **Tokenization**: The process of splitting text into individual words or phrases, known as tokens.\n- **Stopwords**: Common words like \"the\", \"is\", \"in\", which are often removed from text before processing because they add little value to the analysis.\n- **Stemming and Lemmatization**: Techniques to reduce words to their base or root form. Stemming is a crude heuristic process that chops off the ends of words, while lemmatization uses a dictionary to find the root form.\n- **Vectorization**: Converting text into numerical format. Common methods include Bag of Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF).\n- **Word Embeddings**: Dense vector representations of words, capturing their meanings, semantic relationships, and context. Examples include Word2Vec, GloVe, and FastText.\n- **Sequence Models**: Models like Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM), and Gated Recurrent Units (GRUs) that are capable of processing sequences of data.\n- **Transformers**: A type of model architecture that has revolutionized NLP, particularly through models like BERT, GPT, and T5. Transformers handle sequences in parallel and have shown significant improvements in various NLP tasks.\n\n### Structure of This Notebook\n\nThis notebook will take you through a journey from basic NLP tasks to more advanced techniques. Here‚Äôs the structure:\n\n1. **Text Preprocessing**\n   - Tokenization\n   - Stopword removal\n   - Stemming and Lemmatization\n   - Vectorization\n\n2. **Basic NLP Tasks**\n   - Sentiment Analysis\n   - Named Entity Recognition (NER)\n   - Part-of-Speech Tagging\n   - Text Classification\n\n3. **Advanced NLP Techniques**\n   - Word Embeddings (Word2Vec, GloVe)\n   - Sequence Models (RNN, LSTM, GRU)\n   - Attention Mechanisms and Transformers\n   - BERT and other Transformer-based models\n\n4. **Practical Projects**\n   - Building a Sentiment Analysis Model\n   - Creating a Chatbot\n   - Text Summarization\n   - Machine Translation\n\n5. **Conclusion and Further Reading**\n   - Summary of key points\n   - Resources for further learning\n\nLet's get started on this exciting journey into the world of Natural Language Processing!","metadata":{}},{"cell_type":"markdown","source":"# Part 1 : Text Preprocessing","metadata":{}},{"cell_type":"markdown","source":"### üìù Importing libraries","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nimport spacy","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:29.339609Z","iopub.execute_input":"2024-06-13T03:54:29.340026Z","iopub.status.idle":"2024-06-13T03:54:29.346391Z","shell.execute_reply.started":"2024-06-13T03:54:29.339992Z","shell.execute_reply":"2024-06-13T03:54:29.344890Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"#### üöÄ Let's download NLTK and Spacy data we need (just once)","metadata":{}},{"cell_type":"code","source":"!python -m spacy download en_core_web_md","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:29.351746Z","iopub.execute_input":"2024-06-13T03:54:29.352165Z","iopub.status.idle":"2024-06-13T03:54:49.160729Z","shell.execute_reply.started":"2024-06-13T03:54:29.352130Z","shell.execute_reply":"2024-06-13T03:54:49.159161Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Collecting en-core-web-md==3.7.1\n  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl (42.8 MB)\nRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /opt/conda/lib/python3.10/site-packages (from en-core-web-md==3.7.1) (3.7.4)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.2.3)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\nRequirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.3.4)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.9.0)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (6.4.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.66.1)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.31.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.5.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (69.0.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (21.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.0)\nRequirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.26.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.1.1)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.14.6)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2024.2.2)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.11)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.4)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.1.7)\nRequirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.1.3)\n\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_md')\n","output_type":"stream"}]},{"cell_type":"code","source":"nltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\nnlp = spacy.load('en_core_web_sm')\n# nltk.download('omw-1.4')","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:49.163056Z","iopub.execute_input":"2024-06-13T03:54:49.163449Z","iopub.status.idle":"2024-06-13T03:54:50.401769Z","shell.execute_reply.started":"2024-06-13T03:54:49.163412Z","shell.execute_reply":"2024-06-13T03:54:50.400768Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"# üåü Example text\ntext = \"\"\"\nFeatureLens generates visuals by applying text mining concepts such as frequent words, expressions, and closed itemsets of n-grams to guide the discovery process. These concepts are combined with interactive visualization to help users analyze text, create insights\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.403152Z","iopub.execute_input":"2024-06-13T03:54:50.403532Z","iopub.status.idle":"2024-06-13T03:54:50.408926Z","shell.execute_reply.started":"2024-06-13T03:54:50.403482Z","shell.execute_reply":"2024-06-13T03:54:50.407863Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"### ü™Ñ Step 1: Tokenization - Breaking text into words (tokens) üåü","metadata":{}},{"cell_type":"code","source":"tokens = word_tokenize(text)\nprint(\"üîπ Tokens:\", tokens)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.411480Z","iopub.execute_input":"2024-06-13T03:54:50.412532Z","iopub.status.idle":"2024-06-13T03:54:50.423867Z","shell.execute_reply.started":"2024-06-13T03:54:50.412483Z","shell.execute_reply":"2024-06-13T03:54:50.422760Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"üîπ Tokens: ['FeatureLens', 'generates', 'visuals', 'by', 'applying', 'text', 'mining', 'concepts', 'such', 'as', 'frequent', 'words', ',', 'expressions', ',', 'and', 'closed', 'itemsets', 'of', 'n-grams', 'to', 'guide', 'the', 'discovery', 'process', '.', 'These', 'concepts', 'are', 'combined', 'with', 'interactive', 'visualization', 'to', 'help', 'users', 'analyze', 'text', ',', 'create', 'insights']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### üõë Step 2: Stopword removal - Get rid of those pesky common words üõë","metadata":{}},{"cell_type":"code","source":"\nstop_words = set(stopwords.words('english'))\nfiltered_tokens = [word for word in tokens if word.lower() not in stop_words]\nprint(\"üîπ Tokens after stopword removal:\", filtered_tokens)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.425072Z","iopub.execute_input":"2024-06-13T03:54:50.425385Z","iopub.status.idle":"2024-06-13T03:54:50.439646Z","shell.execute_reply.started":"2024-06-13T03:54:50.425357Z","shell.execute_reply":"2024-06-13T03:54:50.438500Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"üîπ Tokens after stopword removal: ['FeatureLens', 'generates', 'visuals', 'applying', 'text', 'mining', 'concepts', 'frequent', 'words', ',', 'expressions', ',', 'closed', 'itemsets', 'n-grams', 'guide', 'discovery', 'process', '.', 'concepts', 'combined', 'interactive', 'visualization', 'help', 'users', 'analyze', 'text', ',', 'create', 'insights']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### ‚úÇÔ∏è Step 3: Stemming - Chopping words to their roots using PorterStemmer! ‚úÇÔ∏è","metadata":{}},{"cell_type":"code","source":"stemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\nprint(\"üîπ Stemmed tokens:\", stemmed_tokens)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.440902Z","iopub.execute_input":"2024-06-13T03:54:50.441224Z","iopub.status.idle":"2024-06-13T03:54:50.457798Z","shell.execute_reply.started":"2024-06-13T03:54:50.441197Z","shell.execute_reply":"2024-06-13T03:54:50.456813Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"üîπ Stemmed tokens: ['featurelen', 'gener', 'visual', 'appli', 'text', 'mine', 'concept', 'frequent', 'word', ',', 'express', ',', 'close', 'itemset', 'n-gram', 'guid', 'discoveri', 'process', '.', 'concept', 'combin', 'interact', 'visual', 'help', 'user', 'analyz', 'text', ',', 'creat', 'insight']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### üå± Step 4: Lemmatization - Morphing words to their base form with WordNetLemmatizer! üå±","metadata":{}},{"cell_type":"code","source":"def lemmatize_with_spacy(tokens):\n    doc = nlp(' '.join(tokens))\n    return [token.lemma_ for token in doc]\n\nlemmatized_tokens = lemmatize_with_spacy(filtered_tokens)\nprint(\"üîπ Lemmatized tokens:\", lemmatized_tokens)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.459011Z","iopub.execute_input":"2024-06-13T03:54:50.459354Z","iopub.status.idle":"2024-06-13T03:54:50.495195Z","shell.execute_reply.started":"2024-06-13T03:54:50.459319Z","shell.execute_reply":"2024-06-13T03:54:50.494118Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"üîπ Lemmatized tokens: ['FeatureLens', 'generate', 'visual', 'apply', 'text', 'mining', 'concept', 'frequent', 'word', ',', 'expression', ',', 'closed', 'itemset', 'n', '-', 'gram', 'guide', 'discovery', 'process', '.', 'concept', 'combine', 'interactive', 'visualization', 'help', 'user', 'analyze', 'text', ',', 'create', 'insight']\n","output_type":"stream"}]},{"cell_type":"code","source":"# lemmatizer = WordNetLemmatizer()\n# lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n# print(\"üîπ Lemmatized tokens:\", lemmatized_tokens)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.496486Z","iopub.execute_input":"2024-06-13T03:54:50.497065Z","iopub.status.idle":"2024-06-13T03:54:50.502877Z","shell.execute_reply.started":"2024-06-13T03:54:50.497034Z","shell.execute_reply":"2024-06-13T03:54:50.501822Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"### üíº Step 5: Vectorization - Turn those words into numbers for our model! üìä","metadata":{}},{"cell_type":"markdown","source":"#### üßÆ Count Vectorizer - Counting word occurrences","metadata":{}},{"cell_type":"code","source":"count_vectorizer = CountVectorizer()\ncount_vectors = count_vectorizer.fit_transform([' '.join(lemmatized_tokens)])\nprint(\"üîπ Count Vectorizer feature names:\", count_vectorizer.get_feature_names_out())\nprint(\"üîπ Count Vectors:\\n\", count_vectors.toarray())","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.504206Z","iopub.execute_input":"2024-06-13T03:54:50.504601Z","iopub.status.idle":"2024-06-13T03:54:50.519183Z","shell.execute_reply.started":"2024-06-13T03:54:50.504564Z","shell.execute_reply":"2024-06-13T03:54:50.517701Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"üîπ Count Vectorizer feature names: ['analyze' 'apply' 'closed' 'combine' 'concept' 'create' 'discovery'\n 'expression' 'featurelens' 'frequent' 'generate' 'gram' 'guide' 'help'\n 'insight' 'interactive' 'itemset' 'mining' 'process' 'text' 'user'\n 'visual' 'visualization' 'word']\nüîπ Count Vectors:\n [[1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### üî• TF-IDF Vectorizer - Considering word frequency across documents","metadata":{}},{"cell_type":"code","source":"tfidf_vectorizer = TfidfVectorizer()\ntfidf_vectors = tfidf_vectorizer.fit_transform([' '.join(lemmatized_tokens)])\nprint(\"üîπ TF-IDF Vectorizer feature names:\", tfidf_vectorizer.get_feature_names_out())\nprint(\"üîπ TF-IDF Vectors:\\n\", tfidf_vectors.toarray())","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.523635Z","iopub.execute_input":"2024-06-13T03:54:50.523994Z","iopub.status.idle":"2024-06-13T03:54:50.537235Z","shell.execute_reply.started":"2024-06-13T03:54:50.523962Z","shell.execute_reply":"2024-06-13T03:54:50.536086Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"üîπ TF-IDF Vectorizer feature names: ['analyze' 'apply' 'closed' 'combine' 'concept' 'create' 'discovery'\n 'expression' 'featurelens' 'frequent' 'generate' 'gram' 'guide' 'help'\n 'insight' 'interactive' 'itemset' 'mining' 'process' 'text' 'user'\n 'visual' 'visualization' 'word']\nüîπ TF-IDF Vectors:\n [[0.18257419 0.18257419 0.18257419 0.18257419 0.36514837 0.18257419\n  0.18257419 0.18257419 0.18257419 0.18257419 0.18257419 0.18257419\n  0.18257419 0.18257419 0.18257419 0.18257419 0.18257419 0.18257419\n  0.18257419 0.36514837 0.18257419 0.18257419 0.18257419 0.18257419]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### üéâ And that's it guys! Our text is now preprocessed and ready for some NLP magic! üéâ","metadata":{}},{"cell_type":"markdown","source":"# Part 2 : Basic NLP Tasks","metadata":{}},{"cell_type":"markdown","source":"####  üéâ Now, let's dive into some basic NLP tasks! üéâ","metadata":{}},{"cell_type":"markdown","source":"### üì¢ Task 1: Sentiment Analysis - Get those text vibes! üòÉüò¢üò°","metadata":{}},{"cell_type":"code","source":"# NLTK's VADER for sentiment analysis\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nsid = SentimentIntensityAnalyzer()\n\nsentiment_scores = sid.polarity_scores(text)\nprint(\"üîπ Sentiment Scores:\", sentiment_scores)\n\ndef get_sentiment_label(scores):\n    if scores['compound'] >= 0.05:\n        return 'positive'\n    elif scores['compound'] <= -0.05:\n        return 'negative'\n    else:\n        return 'neutral'\n    \nsentiment_label = get_sentiment_label(sentiment_scores)\nprint(f\"üîπ Sentiment Label: {sentiment_label}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.538446Z","iopub.execute_input":"2024-06-13T03:54:50.538899Z","iopub.status.idle":"2024-06-13T03:54:50.563092Z","shell.execute_reply.started":"2024-06-13T03:54:50.538858Z","shell.execute_reply":"2024-06-13T03:54:50.561811Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"üîπ Sentiment Scores: {'neg': 0.0, 'neu': 0.879, 'pos': 0.121, 'compound': 0.5859}\nüîπ Sentiment Label: positive\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### üé≠ Task 2: Named Entity Recognition (NER) - Who's who and what's what? ü§î","metadata":{}},{"cell_type":"code","source":"# using spaCy\ndoc = nlp(text)\n\nentities = [(entity.text, entity.label_) for entity in doc.ents]\nprint(\"üîπ Named Entities:\", entities)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.565156Z","iopub.execute_input":"2024-06-13T03:54:50.565573Z","iopub.status.idle":"2024-06-13T03:54:50.592971Z","shell.execute_reply.started":"2024-06-13T03:54:50.565533Z","shell.execute_reply":"2024-06-13T03:54:50.591893Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"üîπ Named Entities: [('FeatureLens', 'ORG')]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### üè∑Ô∏è Task 3: Part-of-Speech Tagging - Tagging words like a pro! üè∑Ô∏è","metadata":{}},{"cell_type":"code","source":"pos_tags = [(token.text, token.pos_) for token in doc]\nprint(\"üîπ Part-of-Speech Tags:\", pos_tags)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.594256Z","iopub.execute_input":"2024-06-13T03:54:50.594605Z","iopub.status.idle":"2024-06-13T03:54:50.600646Z","shell.execute_reply.started":"2024-06-13T03:54:50.594575Z","shell.execute_reply":"2024-06-13T03:54:50.599577Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"üîπ Part-of-Speech Tags: [('\\n', 'SPACE'), ('FeatureLens', 'PROPN'), ('generates', 'VERB'), ('visuals', 'NOUN'), ('by', 'ADP'), ('applying', 'VERB'), ('text', 'NOUN'), ('mining', 'NOUN'), ('concepts', 'NOUN'), ('such', 'ADJ'), ('as', 'ADP'), ('frequent', 'ADJ'), ('words', 'NOUN'), (',', 'PUNCT'), ('expressions', 'NOUN'), (',', 'PUNCT'), ('and', 'CCONJ'), ('closed', 'ADJ'), ('itemsets', 'NOUN'), ('of', 'ADP'), ('n', 'NOUN'), ('-', 'PUNCT'), ('grams', 'NOUN'), ('to', 'PART'), ('guide', 'VERB'), ('the', 'DET'), ('discovery', 'NOUN'), ('process', 'NOUN'), ('.', 'PUNCT'), ('These', 'DET'), ('concepts', 'NOUN'), ('are', 'AUX'), ('combined', 'VERB'), ('with', 'ADP'), ('interactive', 'ADJ'), ('visualization', 'NOUN'), ('to', 'PART'), ('help', 'VERB'), ('users', 'NOUN'), ('analyze', 'VERB'), ('text', 'NOUN'), (',', 'PUNCT'), ('create', 'VERB'), ('insights', 'NOUN'), ('\\n', 'SPACE')]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### üìö Task 4: Text Classification - Let's train a simple classifier! üìö","metadata":{}},{"cell_type":"code","source":"train_texts = [\n    \"I love sunny days\",\n    \"I hate rainy days\",\n    \"Sunshine makes me happy\",\n    \"Rainy days make me sad\"\n]\ntrain_labels = ['positive', 'negative', 'positive', 'negative']","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.602106Z","iopub.execute_input":"2024-06-13T03:54:50.602445Z","iopub.status.idle":"2024-06-13T03:54:50.616930Z","shell.execute_reply.started":"2024-06-13T03:54:50.602410Z","shell.execute_reply":"2024-06-13T03:54:50.615880Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"#### üßë‚Äçüè´ Step 1: Vectorize the training texts","metadata":{}},{"cell_type":"code","source":"vectorizer = CountVectorizer()\nX_train = vectorizer.fit_transform(train_texts)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.618385Z","iopub.execute_input":"2024-06-13T03:54:50.618919Z","iopub.status.idle":"2024-06-13T03:54:50.631422Z","shell.execute_reply.started":"2024-06-13T03:54:50.618880Z","shell.execute_reply":"2024-06-13T03:54:50.630445Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"#### üß† Step 2: Train a simple classifier (Naive Bayes)","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\nclassifier = MultinomialNB()\nclassifier.fit(X_train, train_labels)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.632438Z","iopub.execute_input":"2024-06-13T03:54:50.632799Z","iopub.status.idle":"2024-06-13T03:54:50.650437Z","shell.execute_reply.started":"2024-06-13T03:54:50.632771Z","shell.execute_reply":"2024-06-13T03:54:50.649310Z"},"trusted":true},"execution_count":54,"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"MultinomialNB()","text/html":"<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"markdown","source":"#### üéì Step 3: Predict sentiment of a new text","metadata":{}},{"cell_type":"code","source":"new_text = \"I feel happy when it is sunny\"\nX_new = vectorizer.transform([new_text])\npredicted_label = classifier.predict(X_new)[0]\nprint(f\"üîπ New Text: {new_text}\")\nprint(f\"üîπ Predicted Sentiment: {predicted_label}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.651808Z","iopub.execute_input":"2024-06-13T03:54:50.652176Z","iopub.status.idle":"2024-06-13T03:54:50.663724Z","shell.execute_reply.started":"2024-06-13T03:54:50.652145Z","shell.execute_reply":"2024-06-13T03:54:50.662328Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"üîπ New Text: I feel happy when it is sunny\nüîπ Predicted Sentiment: positive\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### üéâ And that's a wrap on our basic NLP tasks! You're now an NLP hero! üéâ","metadata":{}},{"cell_type":"markdown","source":"# Part 3 : Advanced NLP Techniques","metadata":{}},{"cell_type":"markdown","source":"## Sub-part 1  : Word Embeddings (Word2Vec, GloVe) üåê","metadata":{}},{"cell_type":"markdown","source":"####  üó£Ô∏è Word Embeddings are like word vibes! They capture semantic meaning by representing words as vectors.\n#### üöÄ First up, let's explore Word2Vec using gensim! üß†","metadata":{}},{"cell_type":"code","source":"import gensim\nfrom gensim.models import Word2Vec\n\nsentences = [\n    \"I love sunny days\",\n    \"I hate rainy days\",\n    \"Sunshine makes me happy\",\n    \"Rainy days make me sad\"\n]","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.665022Z","iopub.execute_input":"2024-06-13T03:54:50.665486Z","iopub.status.idle":"2024-06-13T03:54:50.677110Z","shell.execute_reply.started":"2024-06-13T03:54:50.665455Z","shell.execute_reply":"2024-06-13T03:54:50.676050Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"#### ü™Ñ Step 1: Tokenization (we did this before, but let's do it again for completeness)","metadata":{}},{"cell_type":"code","source":"tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\nprint(\"üîπ Tokenized Sentences:\", tokenized_sentences)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.678333Z","iopub.execute_input":"2024-06-13T03:54:50.679017Z","iopub.status.idle":"2024-06-13T03:54:50.690041Z","shell.execute_reply.started":"2024-06-13T03:54:50.678961Z","shell.execute_reply":"2024-06-13T03:54:50.689038Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"üîπ Tokenized Sentences: [['i', 'love', 'sunny', 'days'], ['i', 'hate', 'rainy', 'days'], ['sunshine', 'makes', 'me', 'happy'], ['rainy', 'days', 'make', 'me', 'sad']]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### üß† Step 2: Train the Word2Vec model","metadata":{}},{"cell_type":"code","source":"word2vec_model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\nprint(\"üîπ Word2Vec Model Trained!\")","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.691232Z","iopub.execute_input":"2024-06-13T03:54:50.691600Z","iopub.status.idle":"2024-06-13T03:54:50.713661Z","shell.execute_reply.started":"2024-06-13T03:54:50.691566Z","shell.execute_reply":"2024-06-13T03:54:50.712603Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"üîπ Word2Vec Model Trained!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### üîç Step 3: Explore Word2Vec embeddings","metadata":{}},{"cell_type":"code","source":"word = \"sunny\"\nif word in word2vec_model.wv:\n    print(f\"üîπ Vector for '{word}':\", word2vec_model.wv[word])\nelse:\n    print(f\"üîπ Word '{word}' not in vocabulary\")","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.715013Z","iopub.execute_input":"2024-06-13T03:54:50.715675Z","iopub.status.idle":"2024-06-13T03:54:50.723796Z","shell.execute_reply.started":"2024-06-13T03:54:50.715644Z","shell.execute_reply":"2024-06-13T03:54:50.722731Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"üîπ Vector for 'sunny': [ 7.0887972e-03 -1.5679300e-03  7.9474989e-03 -9.4886590e-03\n -8.0294991e-03 -6.6403709e-03 -4.0034545e-03  4.9892161e-03\n -3.8135587e-03 -8.3199050e-03  8.4117772e-03 -3.7470020e-03\n  8.6086961e-03 -4.8957514e-03  3.9185942e-03  4.9220170e-03\n  2.3926091e-03 -2.8188038e-03  2.8491246e-03 -8.2562361e-03\n -2.7655398e-03 -2.5911583e-03  7.2490061e-03 -3.4634031e-03\n -6.5997029e-03  4.3404270e-03 -4.7448516e-04 -3.5975564e-03\n  6.8824720e-03  3.8723124e-03 -3.9002013e-03  7.7188847e-04\n  9.1435025e-03  7.7546560e-03  6.3618720e-03  4.6673026e-03\n  2.3844899e-03 -1.8416261e-03 -6.3712932e-03 -3.0181051e-04\n -1.5653884e-03 -5.7228567e-04 -6.2628710e-03  7.4340473e-03\n -6.5914928e-03 -7.2392775e-03 -2.7571463e-03 -1.5154004e-03\n -7.6357173e-03  6.9824100e-04 -5.3261113e-03 -1.2755442e-03\n -7.3651113e-03  1.9605684e-03  3.2731986e-03 -2.3138524e-05\n -5.4483581e-03 -1.7260861e-03  7.0849168e-03  3.7362587e-03\n -8.8810492e-03 -3.4135508e-03  2.3541022e-03  2.1380198e-03\n -9.4640078e-03  4.5711659e-03 -8.6569972e-03 -7.3870681e-03\n  3.4831120e-03 -3.4709584e-03  3.5644709e-03  8.8940905e-03\n -3.5743224e-03  9.3204249e-03  1.7110384e-03  9.8477742e-03\n  5.7050432e-03 -9.1494834e-03 -3.3277308e-03  6.5301750e-03\n  5.6027793e-03  8.7055154e-03  6.9261026e-03  8.0388878e-03\n -9.8230084e-03  4.2988253e-03 -5.0300765e-03  3.5123860e-03\n  6.0566878e-03  4.3921317e-03  7.5123594e-03  1.4977157e-03\n -1.2649416e-03  5.7684006e-03 -5.6395675e-03  3.8591625e-05\n  9.4565870e-03 -5.4812501e-03  3.8142789e-03 -8.1130210e-03]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### üëØ‚Äç‚ôÇÔ∏è Find similar words","metadata":{}},{"cell_type":"code","source":"similar_words = word2vec_model.wv.most_similar(word, topn=5)\nprint(f\"üîπ Words similar to '{word}':\", similar_words)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.725469Z","iopub.execute_input":"2024-06-13T03:54:50.725879Z","iopub.status.idle":"2024-06-13T03:54:50.735943Z","shell.execute_reply.started":"2024-06-13T03:54:50.725849Z","shell.execute_reply":"2024-06-13T03:54:50.734729Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"üîπ Words similar to 'sunny': [('love', 0.10941850394010544), ('makes', 0.10889014601707458), ('days', 0.06285077333450317), ('happy', 0.05048205703496933), ('sunshine', 0.026806795969605446)]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### üåê Now, let's explore GloVe embeddings with spaCy! üß†\n#### üì¢ If you are testing this notebook, ,ake sure to download and test the larger spaCy model for better results:\n#### python -m spacy download en_core_web_md","metadata":{}},{"cell_type":"code","source":"nlp_glove = spacy.load('en_core_web_md')","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:50.737246Z","iopub.execute_input":"2024-06-13T03:54:50.737637Z","iopub.status.idle":"2024-06-13T03:54:53.547084Z","shell.execute_reply.started":"2024-06-13T03:54:50.737601Z","shell.execute_reply":"2024-06-13T03:54:53.546002Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"#### üîç Step 4: Explore GloVe embeddings","metadata":{}},{"cell_type":"code","source":"doc = nlp_glove(\"I love sunny days\")\nfor token in doc:\n    print(f\"üîπ Token: {token.text}, Vector: {token.vector[:5]}...\")  # Show first 5 dimensions for brevity","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:53.548469Z","iopub.execute_input":"2024-06-13T03:54:53.548846Z","iopub.status.idle":"2024-06-13T03:54:53.566545Z","shell.execute_reply.started":"2024-06-13T03:54:53.548815Z","shell.execute_reply":"2024-06-13T03:54:53.565387Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"üîπ Token: I, Vector: [ -1.8607    0.15804  -4.1425   -8.6359  -16.955  ]...\nüîπ Token: love, Vector: [ 2.0565  -3.2259  -5.7364  -6.146    0.15748]...\nüîπ Token: sunny, Vector: [-0.14498  0.65651 -2.8764   0.40125 -1.0028 ]...\nüîπ Token: days, Vector: [-3.0808  4.6691  1.1    -1.3048  4.1861]...\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### üëØ‚Äç‚ôÇÔ∏è Find similar words using spaCy's GloVe embeddings","metadata":{}},{"cell_type":"code","source":"def find_similar_words_spacy(word):\n    token = nlp_glove.vocab[word]\n    queries = [w for w in token.vocab if w.is_lower == token.is_lower and w.prob >= -15]\n    by_similarity = sorted(queries, key=lambda w: token.similarity(w), reverse=True)\n    return [w.text for w in by_similarity[:5]]\n\nword = \"sunny\"\nsimilar_words_glove = find_similar_words_spacy(word)\nprint(f\"üîπ Words similar to '{word}' using GloVe:\", similar_words_glove)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:53.567962Z","iopub.execute_input":"2024-06-13T03:54:53.568354Z","iopub.status.idle":"2024-06-13T03:54:53.578756Z","shell.execute_reply.started":"2024-06-13T03:54:53.568319Z","shell.execute_reply":"2024-06-13T03:54:53.577583Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"üîπ Words similar to 'sunny' using GloVe: []\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### üéâ And that's a wrap on Word Embeddings! You're now a Word2Vec and GloVe pro! üéâ","metadata":{}},{"cell_type":"markdown","source":"## Sub-part 2 : Sequence Models (RNN, LSTM, GRU) üåê","metadata":{}},{"cell_type":"markdown","source":"#### üéâ Get ready to ride the wave of Sequence Models! üåä","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, GRU, Dense\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:54:53.580264Z","iopub.execute_input":"2024-06-13T03:54:53.580702Z","iopub.status.idle":"2024-06-13T03:55:07.184396Z","shell.execute_reply.started":"2024-06-13T03:54:53.580653Z","shell.execute_reply":"2024-06-13T03:55:07.182670Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stderr","text":"2024-06-13 03:54:55.730968: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-13 03:54:55.731101: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-13 03:54:55.917712: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# üåü Sample sentences for sequence modeling (replace with your dataset)\nsentences = [\n    \"I love sunny days\",\n    \"I hate rainy days\",\n    \"Sunshine makes me happy\",\n    \"Rainy days make me sad\",\n    \"I feel great on sunny days\",\n    \"Rainy days are gloomy and sad\"\n]","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:55:07.186018Z","iopub.execute_input":"2024-06-13T03:55:07.186892Z","iopub.status.idle":"2024-06-13T03:55:07.192727Z","shell.execute_reply.started":"2024-06-13T03:55:07.186858Z","shell.execute_reply":"2024-06-13T03:55:07.191420Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":"#### üéØ Step 1: Tokenization and Padding","metadata":{}},{"cell_type":"code","source":"max_vocab_size = 1000\nmax_sequence_length = 10\n\n# Tokenize the sentences\ntokenizer = Tokenizer(num_words=max_vocab_size)\ntokenizer.fit_on_texts(sentences)\nsequences = tokenizer.texts_to_sequences(sentences)\nword_index = tokenizer.word_index\n\n# Pad the sequences\npadded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\nprint(\"üîπ Padded Sequences:\\n\", padded_sequences)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:55:07.200793Z","iopub.execute_input":"2024-06-13T03:55:07.201365Z","iopub.status.idle":"2024-06-13T03:55:07.230144Z","shell.execute_reply.started":"2024-06-13T03:55:07.201315Z","shell.execute_reply":"2024-06-13T03:55:07.228787Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"üîπ Padded Sequences:\n [[ 2  7  4  1  0  0  0  0  0  0]\n [ 2  8  3  1  0  0  0  0  0  0]\n [ 9 10  5 11  0  0  0  0  0  0]\n [ 3  1 12  5  6  0  0  0  0  0]\n [ 2 13 14 15  4  1  0  0  0  0]\n [ 3  1 16 17 18  6  0  0  0  0]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### üé¢ Step 2: Preparing the labels","metadata":{}},{"cell_type":"code","source":"labels = np.array([1, 0, 1, 0, 1, 0])  # Positive: 1, Negative: 0","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:55:07.231406Z","iopub.execute_input":"2024-06-13T03:55:07.231767Z","iopub.status.idle":"2024-06-13T03:55:07.244341Z","shell.execute_reply.started":"2024-06-13T03:55:07.231738Z","shell.execute_reply":"2024-06-13T03:55:07.243023Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"#### üöÄ Step 3: Build and train a Simple RNN model","metadata":{}},{"cell_type":"code","source":"print(\"üöÄ Training a Simple RNN model...\")\n\nrnn_model = Sequential([\n    Embedding(input_dim=max_vocab_size, output_dim=32, input_length=max_sequence_length),\n    SimpleRNN(32, return_sequences=False),\n    Dense(1, activation='sigmoid')\n])\n\nrnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nrnn_model.summary()\n\n# Train the RNN model\nrnn_model.fit(padded_sequences, labels, epochs=5, verbose=2)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:55:07.245931Z","iopub.execute_input":"2024-06-13T03:55:07.246379Z","iopub.status.idle":"2024-06-13T03:55:09.804385Z","shell.execute_reply.started":"2024-06-13T03:55:07.246340Z","shell.execute_reply":"2024-06-13T03:55:09.803288Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"üöÄ Training a Simple RNN model...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:86: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           ‚îÇ ?                      ‚îÇ   \u001b[38;5;34m0\u001b[0m (unbuilt) ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)          ‚îÇ ?                      ‚îÇ   \u001b[38;5;34m0\u001b[0m (unbuilt) ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense (\u001b[38;5;33mDense\u001b[0m)                   ‚îÇ ?                      ‚îÇ   \u001b[38;5;34m0\u001b[0m (unbuilt) ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ<span style=\"font-weight: bold\"> Layer (type)                    </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape           </span>‚îÉ<span style=\"font-weight: bold\">       Param # </span>‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           ‚îÇ ?                      ‚îÇ   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)          ‚îÇ ?                      ‚îÇ   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   ‚îÇ ?                      ‚îÇ   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/5\n1/1 - 2s - 2s/step - accuracy: 0.3333 - loss: 0.7038\nEpoch 2/5\n1/1 - 0s - 26ms/step - accuracy: 0.6667 - loss: 0.6854\nEpoch 3/5\n1/1 - 0s - 26ms/step - accuracy: 1.0000 - loss: 0.6676\nEpoch 4/5\n1/1 - 0s - 26ms/step - accuracy: 1.0000 - loss: 0.6498\nEpoch 5/5\n1/1 - 0s - 27ms/step - accuracy: 1.0000 - loss: 0.6317\n","output_type":"stream"},{"execution_count":68,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7e7ec31e9ed0>"},"metadata":{}}]},{"cell_type":"markdown","source":"#### üöÄ Step 4: Build and train an LSTM model","metadata":{}},{"cell_type":"code","source":"print(\"üöÄ Training an LSTM model...\")\n\nlstm_model = Sequential([\n    Embedding(input_dim=max_vocab_size, output_dim=32, input_length=max_sequence_length),\n    LSTM(32, return_sequences=False),\n    Dense(1, activation='sigmoid')\n])\n\nlstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nlstm_model.summary()\n\n# Train the LSTM model\nlstm_model.fit(padded_sequences, labels, epochs=5, verbose=2)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:55:09.805960Z","iopub.execute_input":"2024-06-13T03:55:09.806398Z","iopub.status.idle":"2024-06-13T03:55:12.428163Z","shell.execute_reply.started":"2024-06-13T03:55:09.806360Z","shell.execute_reply":"2024-06-13T03:55:12.427040Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stdout","text":"üöÄ Training an LSTM model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         ‚îÇ ?                      ‚îÇ   \u001b[38;5;34m0\u001b[0m (unbuilt) ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     ‚îÇ ?                      ‚îÇ   \u001b[38;5;34m0\u001b[0m (unbuilt) ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 ‚îÇ ?                      ‚îÇ   \u001b[38;5;34m0\u001b[0m (unbuilt) ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ<span style=\"font-weight: bold\"> Layer (type)                    </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape           </span>‚îÉ<span style=\"font-weight: bold\">       Param # </span>‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         ‚îÇ ?                      ‚îÇ   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     ‚îÇ ?                      ‚îÇ   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 ‚îÇ ?                      ‚îÇ   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/5\n1/1 - 2s - 2s/step - accuracy: 0.5000 - loss: 0.6931\nEpoch 2/5\n1/1 - 0s - 28ms/step - accuracy: 0.5000 - loss: 0.6926\nEpoch 3/5\n1/1 - 0s - 28ms/step - accuracy: 0.6667 - loss: 0.6921\nEpoch 4/5\n1/1 - 0s - 28ms/step - accuracy: 1.0000 - loss: 0.6915\nEpoch 5/5\n1/1 - 0s - 27ms/step - accuracy: 0.6667 - loss: 0.6910\n","output_type":"stream"},{"execution_count":69,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7e7ebe52d1b0>"},"metadata":{}}]},{"cell_type":"markdown","source":"#### üöÄ Step 5: Build and train a GRU model","metadata":{}},{"cell_type":"code","source":"print(\"üöÄ Training a GRU model...\")\n\ngru_model = Sequential([\n    Embedding(input_dim=max_vocab_size, output_dim=32, input_length=max_sequence_length),\n    GRU(32, return_sequences=False),\n    Dense(1, activation='sigmoid')\n])\n\ngru_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\ngru_model.summary()\n\ngru_model.fit(padded_sequences, labels, epochs=5, verbose=2)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:55:12.429611Z","iopub.execute_input":"2024-06-13T03:55:12.429976Z","iopub.status.idle":"2024-06-13T03:55:15.357754Z","shell.execute_reply.started":"2024-06-13T03:55:12.429946Z","shell.execute_reply":"2024-06-13T03:55:15.356580Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"üöÄ Training a GRU model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_2\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         ‚îÇ ?                      ‚îÇ   \u001b[38;5;34m0\u001b[0m (unbuilt) ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ gru (\u001b[38;5;33mGRU\u001b[0m)                       ‚îÇ ?                      ‚îÇ   \u001b[38;5;34m0\u001b[0m (unbuilt) ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 ‚îÇ ?                      ‚îÇ   \u001b[38;5;34m0\u001b[0m (unbuilt) ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ<span style=\"font-weight: bold\"> Layer (type)                    </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape           </span>‚îÉ<span style=\"font-weight: bold\">       Param # </span>‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         ‚îÇ ?                      ‚îÇ   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       ‚îÇ ?                      ‚îÇ   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 ‚îÇ ?                      ‚îÇ   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/5\n1/1 - 3s - 3s/step - accuracy: 0.5000 - loss: 0.6930\nEpoch 2/5\n1/1 - 0s - 29ms/step - accuracy: 0.5000 - loss: 0.6928\nEpoch 3/5\n1/1 - 0s - 59ms/step - accuracy: 0.5000 - loss: 0.6925\nEpoch 4/5\n1/1 - 0s - 28ms/step - accuracy: 0.8333 - loss: 0.6923\nEpoch 5/5\n1/1 - 0s - 28ms/step - accuracy: 0.6667 - loss: 0.6920\n","output_type":"stream"},{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7e7ec0498d60>"},"metadata":{}}]},{"cell_type":"markdown","source":"#### üéâ Comparing Model Performances","metadata":{}},{"cell_type":"code","source":"# Evaluate the models\nrnn_loss, rnn_accuracy = rnn_model.evaluate(padded_sequences, labels, verbose=0)\nlstm_loss, lstm_accuracy = lstm_model.evaluate(padded_sequences, labels, verbose=0)\ngru_loss, gru_accuracy = gru_model.evaluate(padded_sequences, labels, verbose=0)\n\nprint(f\"üîπ RNN Model Accuracy: {rnn_accuracy:.2f}\")\nprint(f\"üîπ LSTM Model Accuracy: {lstm_accuracy:.2f}\")\nprint(f\"üîπ GRU Model Accuracy: {gru_accuracy:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:55:15.359401Z","iopub.execute_input":"2024-06-13T03:55:15.359879Z","iopub.status.idle":"2024-06-13T03:55:16.408653Z","shell.execute_reply.started":"2024-06-13T03:55:15.359839Z","shell.execute_reply":"2024-06-13T03:55:16.407274Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":"üîπ RNN Model Accuracy: 1.00\nüîπ LSTM Model Accuracy: 0.67\nüîπ GRU Model Accuracy: 0.67\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### üéâ And that's a wrap on Sequence Models! You're now an RNN, LSTM, and GRU pro! üéâ","metadata":{}}]}